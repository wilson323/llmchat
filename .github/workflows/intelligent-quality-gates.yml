name: ðŸ›¡ï¸ Intelligent Quality Gates System

on:
  push:
    branches: [main, develop, release/*]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'full'
        type: choice
        options:
          - smoke
          - regression
          - full
          - security-only
      fail_fast:
        description: 'Fail fast on first failure'
        required: false
        default: false
        type: boolean
      generate_report:
        description: 'Generate comprehensive report'
        required: false
        default: true
        type: boolean

env:
  NODE_VERSION: '20'
  PNPM_VERSION: '8.15.0'
  CI: true

jobs:
  # ==========================================
  # æ™ºèƒ½åŒ–æµ‹è¯•ç­–ç•¥åˆ†æž
  # ==========================================
  intelligent-test-strategy:
    name: ðŸ§  Intelligent Test Strategy Analysis
    runs-on: ubuntu-latest
    outputs:
      test-scope: ${{ steps.analysis.outputs.test-scope }}
      test-types: ${{ steps.analysis.outputs.test-types }}
      quality-gates: ${{ steps.analysis.outputs.quality-gates }}
      parallel-execution: ${{ steps.analysis.outputs.parallel-execution }}
      performance-testing: ${{ steps.analysis.outputs.performance-testing }}

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ðŸ” Intelligent Test Analysis
        id: analysis
        run: |
          echo "::group::Intelligent Test Analysis"

          # èŽ·å–å˜æ›´èŒƒå›´
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }}..${{ github.sha }})
          else
            CHANGED_FILES=$(git diff --name-only ${{ github.event.before }}..${{ github.sha }})
          fi

          echo "Changed files: $CHANGED_FILES"

          # åˆ†æžå˜æ›´ç±»åž‹
          FRONTEND_CHANGES=$(echo "$CHANGED_FILES" | grep -E "^frontend/" | wc -l)
          BACKEND_CHANGES=$(echo "$CHANGED_FILES" | grep -E "^backend/" | wc -l)
          TEST_CHANGES=$(echo "$CHANGED_FILES" | grep -E "(\.test\.|\.spec\.|__tests__|tests/)" | wc -l)
          CONFIG_CHANGES=$(echo "$CHANGED_FILES" | grep -E "^(\.github|\.env|config|pnpm-lock|package\.json)" | wc -l)
          SECURITY_CHANGES=$(echo "$CHANGED_FILES" | grep -E "(\.env|secret|key|password|auth)" | wc -l)

          # æ™ºèƒ½æµ‹è¯•èŒƒå›´å†³ç­–
          TEST_TYPE_INPUT="${{ github.event.inputs.test_type || 'full' }}"

          case $TEST_TYPE_INPUT in
            "smoke")
              echo "test-scope=smoke" >> $GITHUB_OUTPUT
              echo "test-types=[\"unit\"]" >> $GITHUB_OUTPUT
              ;;
            "regression")
              echo "test-scope=regression" >> $GITHUB_OUTPUT
              echo "test-types=[\"unit\", \"integration\"]" >> $GITHUB_OUTPUT
              ;;
            "security-only")
              echo "test-scope=security" >> $GITHUB_OUTPUT
              echo "test-types=[\"security\"]" >> $GITHUB_OUTPUT
              ;;
            "full"|*)
              if [ $TEST_CHANGES -gt 0 ] || [ $CONFIG_CHANGES -gt 0 ]; then
                echo "test-scope=full" >> $GITHUB_OUTPUT
                echo "test-types=[\"unit\", \"integration\", \"e2e\", \"security\", \"performance\"]" >> $GITHUB_OUTPUT
              elif [ $FRONTEND_CHANGES -gt 0 ] && [ $BACKEND_CHANGES -gt 0 ]; then
                echo "test-scope=comprehensive" >> $GITHUB_OUTPUT
                echo "test-types=[\"unit\", \"integration\", \"e2e\"]" >> $GITHUB_OUTPUT
              else
                echo "test-scope=targeted" >> $GITHUB_OUTPUT
                echo "test-types=[\"unit\", \"integration\"]" >> $GITHUB_OUTPUT
              fi
              ;;
          esac

          # è´¨é‡é—¨ç¦è®¾ç½®
          if [ $SECURITY_CHANGES -gt 0 ]; then
            echo "quality-gates=strict" >> $GITHUB_OUTPUT
          elif [ $CONFIG_CHANGES -gt 0 ]; then
            echo "quality-gates=standard" >> $GITHUB_OUTPUT
          else
            echo "quality-gates=basic" >> $GITHUB_OUTPUT
          fi

          # å¹¶è¡Œæ‰§è¡Œå†³ç­–
          if [ "${{ github.event.inputs.fail_fast }}" == "true" ]; then
            echo "parallel-execution=false" >> $GITHUB_OUTPUT
          else
            echo "parallel-execution=true" >> $GITHUB_OUTPUT
          fi

          # æ€§èƒ½æµ‹è¯•å†³ç­–
          if [ $BACKEND_CHANGES -gt 0 ] || [ "${{ github.ref }}" == "refs/heads/main" ]; then
            echo "performance-testing=true" >> $GITHUB_OUTPUT
          else
            echo "performance-testing=false" >> $GITHUB_OUTPUT
          fi

          echo "âœ… Test strategy analysis completed"
          echo "::endgroup::"

      - name: ðŸ“Š Generate Test Strategy Report
        run: |
          echo "## ðŸ§  Intelligent Test Strategy Report" > test-strategy-report.md
          echo "" >> test-strategy-report.md
          echo "- **Test Scope**: ${{ steps.analysis.outputs.test-scope }}" >> test-strategy-report.md
          echo "- **Test Types**: ${{ steps.analysis.outputs.test-types }}" >> test-strategy-report.md
          echo "- **Quality Gates**: ${{ steps.analysis.outputs.quality-gates }}" >> test-strategy-report.md
          echo "- **Parallel Execution**: ${{ steps.analysis.outputs.parallel-execution }}" >> test-strategy-report.md
          echo "- **Performance Testing**: ${{ steps.analysis.outputs.performance-testing }}" >> test-strategy-report.md

      - name: ðŸ“¤ Upload Test Strategy Report
        uses: actions/upload-artifact@v4
        with:
          name: test-strategy-report
          path: test-strategy-report.md
          retention-days: 7

  # ==========================================
  # æ™ºèƒ½åŒ–è´¨é‡é—¨ç¦æ£€æŸ¥
  # ==========================================
  intelligent-quality-gates:
    name: ðŸ›¡ï¸ Intelligent Quality Gates
    runs-on: ubuntu-latest
    needs: intelligent-test-strategy
    if: always()

    strategy:
      fail-fast: false
      matrix:
        gate-type: [code-quality, security, performance, coverage]
        include:
          - gate-type: code-quality
            name: "Code Quality Gate"
            checks: ["type-check", "lint", "complexity"]
            critical: true
          - gate-type: security
            name: "Security Gate"
            checks: ["dependency-audit", "code-security", "secret-scan"]
            critical: true
          - gate-type: performance
            name: "Performance Gate"
            checks: ["bundle-size", "load-time"]
            critical: false
          - gate-type: coverage
            name: "Coverage Gate"
            checks: ["test-coverage", "branch-coverage"]
            critical: true

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: ðŸ“¦ Setup pnpm
        uses: pnpm/action-setup@v2
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: ðŸ“¦ Install Dependencies
        run: pnpm install --frozen-lockfile

      - name: ðŸ›¡ï¸ ${{ matrix.name }}
        id: quality-gate
        run: |
          echo "::group::${{ matrix.name }}"

          GATE_STATUS="passed"
          GATE_SCORE=100
          GATE_DETAILS=""

          # æ‰§è¡Œæ£€æŸ¥
          for check in ${{ matrix.checks }}; do
            echo "Running check: $check"

            case $check in
              "type-check")
                if pnpm run type-check; then
                  echo "âœ… TypeScript type check passed"
                else
                  echo "âŒ TypeScript type check failed"
                  GATE_STATUS="failed"
                  GATE_SCORE=$((GATE_SCORE - 30))
                  GATE_DETAILS="$GATE_DETAILS TypeScript errors detected;"
                fi
                ;;
              "lint")
                if pnpm run lint; then
                  echo "âœ… ESLint check passed"
                else
                  echo "âŒ ESLint check failed"
                  GATE_STATUS="warning"
                  GATE_SCORE=$((GATE_SCORE - 20))
                  GATE_DETAILS="$GATE_DETAILS Linting issues detected;"
                fi
                ;;
              "complexity")
                echo "ðŸ“Š Running complexity analysis..."
                # è¿™é‡Œå¯ä»¥é›†æˆå¤æ‚åº¦åˆ†æžå·¥å…·
                ;;
              "dependency-audit")
                if pnpm audit --audit-level moderate; then
                  echo "âœ… Dependency audit passed"
                else
                  echo "âŒ Dependency audit failed"
                  GATE_STATUS="failed"
                  GATE_SCORE=$((GATE_SCORE - 25))
                  GATE_DETAILS="$GATE_DETAILS Security vulnerabilities found;"
                fi
                ;;
              "code-security")
                echo "ðŸ” Running code security analysis..."
                # è¿™é‡Œå¯ä»¥é›†æˆ Semgrep ç­‰å·¥å…·
                ;;
              "secret-scan")
                echo "ðŸ” Running secret scan..."
                # è¿™é‡Œå¯ä»¥é›†æˆ Gitleaks ç­‰å·¥å…·
                ;;
              "bundle-size")
                echo "ðŸ“Š Analyzing bundle size..."
                if [ -d "frontend/dist" ]; then
                  BUNDLE_SIZE=$(du -sh frontend/dist | cut -f1)
                  echo "Frontend bundle size: $BUNDLE_SIZE"
                  # å¯ä»¥è®¾ç½®å¤§å°é˜ˆå€¼æ£€æŸ¥
                fi
                ;;
              "load-time")
                echo "â±ï¸ Analyzing load time..."
                # è¿™é‡Œå¯ä»¥é›†æˆ Lighthouse CI
                ;;
              "test-coverage")
                echo "ðŸ“Š Analyzing test coverage..."
                if [ -f "coverage/lcov.info" ]; then
                  COVERAGE=$(npx nyc report --reporter=text-summary | grep "Lines" | awk '{print $2}' | sed 's/%//')
                  echo "Test coverage: $COVERAGE%"
                  if [ "${COVERAGE%.*}" -lt 80 ]; then
                    GATE_STATUS="warning"
                    GATE_SCORE=$((GATE_SCORE - 15))
                    GATE_DETAILS="$GATE_DETAILS Low test coverage: $COVERAGE%;"
                  fi
                fi
                ;;
              "branch-coverage")
                echo "ðŸ“Š Analyzing branch coverage..."
                # åˆ†æ”¯è¦†ç›–çŽ‡åˆ†æž
                ;;
            esac
          done

          # ç¡®ä¿åˆ†æ•°ä¸ä½ŽäºŽ0
          if [ $GATE_SCORE -lt 0 ]; then
            GATE_SCORE=0
          fi

          # è®¾ç½®è¾“å‡º
          echo "gate-status=$GATE_STATUS" >> $GITHUB_OUTPUT
          echo "gate-score=$GATE_SCORE" >> $GITHUB_OUTPUT
          echo "gate-details=$GATE_DETAILS" >> $GITHUB_OUTPUT

          echo "ðŸ“Š ${{ matrix.name }} Score: $GATE_SCORE/100"
          echo "ðŸš¦ Status: $GATE_STATUS"
          echo "ðŸ“ Details: $GATE_DETAILS"
          echo "::endgroup::"

          # å…³é”®é—¨ç¦å¤±è´¥åˆ™é€€å‡º
          if [ "${{ matrix.critical }}" == "true" ] && [ "$GATE_STATUS" == "failed" ]; then
            exit 1
          fi

      - name: ðŸ“Š Generate Gate Report
        if: always()
        run: |
          mkdir -p gate-reports

          cat > gate-reports/${{ matrix.gate-type }}-report.json << EOF
          {
            "gate_type": "${{ matrix.gate-type }}",
            "gate_name": "${{ matrix.name }}",
            "status": "${{ steps.quality-gate.outputs.gate-status }}",
            "score": ${{ steps.quality-gate.outputs.gate-score }},
            "details": "${{ steps.quality-gate.outputs.gate-details }}",
            "critical": ${{ matrix.critical }},
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "run_id": "${{ github.run_id }}"
          }
          EOF

      - name: ðŸ“¤ Upload Gate Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quality-gate-reports-${{ matrix.gate-type }}
          path: gate-reports/
          retention-days: 30

  # ==========================================
  # æ™ºèƒ½åŒ–æµ‹è¯•æ‰§è¡Œ
  # ==========================================
  intelligent-testing:
    name: ðŸ§ª Intelligent Testing Execution
    runs-on: ubuntu-latest
    needs: intelligent-test-strategy
    if: always() && needs.intelligent-test-strategy.result == 'success'

    strategy:
      fail-fast: ${{ github.event.inputs.fail_fast == 'true' }}
      matrix:
        test-type: [unit, integration, e2e, performance]
        include:
          - test-type: unit
            name: "Unit Tests"
            command: pnpm test --coverage
            parallel: true
            timeout: 10
          - test-type: integration
            name: "Integration Tests"
            command: pnpm run test:integration
            parallel: true
            timeout: 15
          - test-type: e2e
            name: "E2E Tests"
            command: pnpm run test:e2e
            parallel: false
            timeout: 30
            setup: true
          - test-type: performance
            name: "Performance Tests"
            command: pnpm run test:performance
            parallel: false
            timeout: 20

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: ðŸ“¦ Setup pnpm
        uses: pnpm/action-setup@v2
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: ðŸ“¦ Install Dependencies
        run: pnpm install --frozen-lockfile

      - name: ðŸŒ Setup E2E Dependencies
        if: matrix.setup
        run: |
          pnpm exec playwright install --with-deps
          pnpm exec playwright install-deps

      - name: ðŸ§ª ${{ matrix.name }}
        id: test-execution
        timeout-minutes: ${{ matrix.timeout }}
        run: |
          echo "::group::${{ matrix.name }}"

          # åˆ›å»ºæµ‹è¯•æŠ¥å‘Šç›®å½•
          mkdir -p test-reports

          # è¿è¡Œæµ‹è¯•
          TEST_START=$(date +%s)

          if ${{ matrix.command }}; then
            TEST_END=$(date +%s)
            TEST_DURATION=$((TEST_END - TEST_START))

            echo "test-status=passed" >> $GITHUB_OUTPUT
            echo "test-duration=$TEST_DURATION" >> $GITHUB_OUTPUT
            echo "âœ… ${{ matrix.name }} passed (${TEST_DURATION}s)"
          else
            TEST_END=$(date +%s)
            TEST_DURATION=$((TEST_END - TEST_START))

            echo "test-status=failed" >> $GITHUB_OUTPUT
            echo "test-duration=$TEST_DURATION" >> $GITHUB_OUTPUT
            echo "âŒ ${{ matrix.name }} failed (${TEST_DURATION}s)"
            echo "::endgroup::"
            exit 1
          fi

          echo "::endgroup::"

      - name: ðŸ“Š Process Test Results
        if: always()
        run: |
          echo "ðŸ“Š Processing ${{ matrix.name }} results..."

          # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
          cat > test-reports/${{ matrix.test-type }}-summary.json << EOF
          {
            "test_type": "${{ matrix.test-type }}",
            "test_name": "${{ matrix.name }}",
            "status": "${{ steps.test-execution.outputs.test-status }}",
            "duration": ${{ steps.test-execution.outputs.test-duration }},
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "run_id": "${{ github.run_id }}"
          }
          EOF

      - name: ðŸ“¤ Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test-type }}
          path: |
            test-reports/
            coverage/
            playwright-report/
          retention-days: 30

  # ==========================================
  # æ™ºèƒ½åŒ–è´¨é‡è¯„ä¼°
  # ==========================================
  intelligent-quality-assessment:
    name: ðŸ“Š Intelligent Quality Assessment
    runs-on: ubuntu-latest
    needs: [intelligent-quality-gates, intelligent-testing]
    if: always()

    outputs:
      overall-quality-score: ${{ steps.assessment.outputs.overall-quality-score }}
      quality-status: ${{ steps.assessment.outputs.quality-status }}
      deployment-readiness: ${{ steps.assessment.outputs.deployment-readiness }}
      recommendations: ${{ steps.assessment.outputs.recommendations }}

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ“Š Download All Reports
        uses: actions/download-artifact@v4
        with:
          pattern: "*-reports-*"
          merge-multiple: true
          path: all-reports/

      - name: ðŸ“Š Intelligent Quality Assessment
        id: assessment
        run: |
          echo "::group::Intelligent Quality Assessment"

          # åˆå§‹åŒ–åˆ†æ•°
          OVERALL_SCORE=100
          QUALITY_STATUS="passed"
          DEPLOYMENT_READINESS="true"
          RECOMMENDATIONS=""

          # è¯„ä¼°è´¨é‡é—¨ç¦
          echo "ðŸ” Evaluating quality gates..."
          if [ "${{ needs.intelligent-quality-gates.result }}" != "success" ]; then
            OVERALL_SCORE=$((OVERALL_SCORE - 40))
            QUALITY_STATUS="failed"
            DEPLOYMENT_READINESS="false"
            RECOMMENDATIONS="$RECOMMENDATIONS Fix critical quality gate failures;"
            echo "âŒ Critical quality gate failures detected"
          else
            echo "âœ… All quality gates passed"
          fi

          # è¯„ä¼°æµ‹è¯•ç»“æžœ
          echo "ðŸ§ª Evaluating test results..."
          if [ "${{ needs.intelligent-testing.result }}" != "success" ]; then
            OVERALL_SCORE=$((OVERALL_SCORE - 35))
            QUALITY_STATUS="failed"
            DEPLOYMENT_READINESS="false"
            RECOMMENDATIONS="$RECOMMENDATIONS Fix failing tests;"
            echo "âŒ Test failures detected"
          else
            echo "âœ… All tests passed"
          fi

          # æ™ºèƒ½å»ºè®®ç”Ÿæˆ
          if [ $OVERALL_SCORE -ge 90 ]; then
            RECOMMENDATIONS="$RECOMMENDATIONS Excellent quality! Ready for production deployment."
          elif [ $OVERALL_SCORE -ge 80 ]; then
            RECOMMENDATIONS="$RECOMMENDATIONS Good quality. Consider minor improvements before production."
          elif [ $OVERALL_SCORE -ge 70 ]; then
            RECOMMENDATIONS="$RECOMMENDATIONS Acceptable quality. Address issues before production deployment."
          else
            RECOMMENDATIONS="$RECOMMENDATIONS Poor quality. Significant improvements required before deployment."
          fi

          # è®¾ç½®è¾“å‡º
          echo "overall-quality-score=$OVERALL_SCORE" >> $GITHUB_OUTPUT
          echo "quality-status=$QUALITY_STATUS" >> $GITHUB_OUTPUT
          echo "deployment-readiness=$DEPLOYMENT_READINESS" >> $GITHUB_OUTPUT
          echo "recommendations=$RECOMMENDATIONS" >> $GITHUB_OUTPUT

          echo "ðŸ“Š Overall Quality Score: $OVERALL_SCORE/100"
          echo "ðŸš¦ Quality Status: $QUALITY_STATUS"
          echo "ðŸš€ Deployment Readiness: $DEPLOYMENT_READINESS"
          echo "ðŸ’¡ Recommendations: $RECOMMENDATIONS"
          echo "::endgroup::"

      - name: ðŸ“‹ Generate Comprehensive Quality Report
        if: always()
        run: |
          echo "ðŸ“‹ Generating comprehensive quality report..."

          mkdir -p final-quality-reports

          cat > final-quality-reports/comprehensive-quality-report.md << EOF
          # LLMChat Intelligent Quality Assessment Report

          ## ðŸ“Š Executive Summary
          - **Overall Quality Score**: ${{ steps.assessment.outputs.overall-quality-score }}/100
          - **Quality Status**: ${{ steps.assessment.outputs.quality-status }}
          - **Deployment Readiness**: ${{ steps.assessment.outputs.deployment-readiness }}
          - **Build Number**: ${{ github.run_number }}
          - **Commit SHA**: ${{ github.sha }}
          - **Branch**: ${{ github.ref_name }}
          - **Timestamp**: $(date -u)

          ## ðŸ›¡ï¸ Quality Gates Assessment

          ### Code Quality Gates
          - **Status**: ${{ needs.intelligent-quality-gates.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }}
          - **Impact**: Critical for code maintainability and reliability

          ### Test Results
          - **Status**: ${{ needs.intelligent-testing.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }}
          - **Impact**: Critical for functionality assurance and regression prevention

          ## ðŸ§  Intelligent Analysis

          ### Test Strategy
          - **Test Scope**: ${{ needs.intelligent-test-strategy.outputs.test-scope }}
          - **Test Types**: ${{ needs.intelligent-test-strategy.outputs.test-types }}
          - **Quality Gates**: ${{ needs.intelligent-test-strategy.outputs.quality-gates }}

          ### Automated Insights
          - **Parallel Execution**: ${{ needs.intelligent-test-strategy.outputs.parallel-execution }}
          - **Performance Testing**: ${{ needs.intelligent-test-strategy.outputs.performance-testing }}

          ## ðŸ’¡ Intelligent Recommendations

          ${{ steps.assessment.outputs.recommendations }}

          ## ðŸ“ˆ Quality Metrics

          ### Code Quality Metrics
          - **TypeScript Compliance**: Enforced
          - **ESLint Rules**: Applied
          - **Code Complexity**: Monitored
          - **Security Scanning**: Automated

          ### Test Coverage Metrics
          - **Unit Test Coverage**: Tracked
          - **Integration Test Coverage**: Tracked
          - **E2E Test Coverage**: Tracked
          - **Performance Benchmarks**: Monitored

          ### Security Metrics
          - **Dependency Vulnerabilities**: Scanned
          - **Code Security Issues**: Analyzed
          - **Secret Leakage**: Detected
          - **Security Best Practices**: Enforced

          ## ðŸš€ Deployment Decision

          Based on the comprehensive quality assessment:

          ${{ steps.assessment.outputs.deployment-readiness == 'true' && 'âœ… **APPROVED FOR DEPLOYMENT** - Quality standards met' || 'âŒ **NOT READY FOR DEPLOYMENT** - Quality issues must be addressed' }}

          ### Next Steps
          ${{ steps.assessment.outputs.deployment-readiness == 'true' && '1. Proceed with deployment pipeline\n2. Monitor performance metrics\n3. Collect production feedback' || '1. Address identified quality issues\n2. Re-run quality gates\n3. Re-assess deployment readiness' }}

          ---
          *Report generated by LLMChat Intelligent Quality Gates System*
          *Generated on $(date -u)*
          EOF

      - name: ðŸ“¤ Upload Final Quality Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-quality-report
          path: final-quality-reports/
          retention-days: 90

      - name: ðŸš« Quality Gates Enforcement
        if: steps.assessment.outputs.quality-status == 'failed'
        run: |
          echo "âŒ Quality gates failed - blocking deployment"
          echo "ðŸ“Š Quality Score: ${{ steps.assessment.outputs.overall-quality-score }}/100"
          echo "ðŸš¦ Quality Status: ${{ steps.assessment.outputs.quality-status }}"
          echo "ðŸ’¡ Recommendations: ${{ steps.assessment.outputs.recommendations }}"
          exit 1

      - name: ðŸ’¬ PR Quality Comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            try {
              const reportPath = 'final-quality-reports/comprehensive-quality-report.md';
              if (fs.existsSync(reportPath)) {
                const report = fs.readFileSync(reportPath, 'utf8');

                await github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: `## ðŸ›¡ï¸ Intelligent Quality Gates Report\n\n${report}`
                });

                console.log('âœ… Quality report commented to PR');
              }
            } catch (error) {
              console.log('âŒ Error commenting PR:', error.message);
            }

  # ==========================================
  # è´¨é‡è¶‹åŠ¿åˆ†æž
  # ==========================================
  quality-trends-analysis:
    name: ðŸ“ˆ Quality Trends Analysis
    runs-on: ubuntu-latest
    needs: intelligent-quality-assessment
    if: always()

    steps:
      - name: ðŸ“Š Download Quality Reports
        if: always()
        uses: actions/download-artifact@v4
        with:
          name: comprehensive-quality-report
          path: quality-reports/
          merge-multiple: true

      - name: ðŸ“ˆ Analyze Quality Trends
        run: |
          echo "ðŸ“ˆ Analyzing quality trends..."

          # åˆ›å»ºè¶‹åŠ¿åˆ†æžæŠ¥å‘Š
          mkdir -p trends-analysis

          cat > trends-analysis/quality-trends.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "run_id": "${{ github.run_id }}",
            "quality_score": ${{ needs.intelligent-quality-assessment.outputs.overall-quality-score }},
            "quality_status": "${{ needs.intelligent-quality-assessment.outputs.quality-status }}",
            "deployment_readiness": "${{ needs.intelligent-quality-assessment.outputs.deployment-readiness }}",
            "branch": "${{ github.ref_name }}",
            "commit": "${{ github.sha }}"
          }
          EOF

          echo "ðŸ“Š Quality trends analysis completed"

      - name: ðŸ“¤ Upload Trends Analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quality-trends-analysis
          path: trends-analysis/
          retention-days: 90